---
title: "BigModel Introduction"
description: "Zhipu AI Open Platform Introduction"
---

# BigModel Introduction

Zhipu AI Open Platform provides a series of large models with different functions and pricing, including general-purpose large models, hyper-realistic large models, image large models, vector large models, etc., and supports fine-tuning models with your private data.

On January 16, 2024, we launched a new generation of foundation large model GLM-4 at the "Zhipu AI Technology Open Day (ZHIPU DevDay)".

## Resources

- View model API documentation
- Experience model capabilities in the Experience Center
- Check your API Key
- Create knowledge bases
- Create applications in the Application Center

## Key Concepts

### GLM

GLM stands for General Language Model, which is a pre-trained language model based on autoregressive gap-filling. The ChatGLM series models support relatively complex natural language instructions and can solve difficult reasoning problems. The model is equipped with easy-to-use API interfaces, allowing developers to easily integrate it into various applications, widely used in intelligent customer service, virtual hosts, chatbots, and many other fields.

### Embedding

Embedding is a method of converting data (such as text) into vector form. This representation ensures that data similar in certain aspects are close to each other in the vector space, while unrelated data are further apart. By converting text strings into vectors, data can be effectively used for applications such as search, clustering, recommendation systems, anomaly detection, and classification.

### Token

A token is the basic unit used by models to represent natural language text, which can be intuitively understood as "characters" or "words"; typically 1 Chinese word, 1 English word, 1 number, or 1 symbol counts as 1 token.

In the GLM series models, the conversion ratio between tokens and characters is approximately 1:1.6. However, due to different tokenization in different models, the conversion ratio may vary. The actual number of tokens processed each time is based on the model's return, which you can check from the usage information in the response.

### Context

The context window refers to the maximum length that a model can process in a single conversation. This includes:

- User input
- Model-generated responses
- Intermediate content produced by the model during inference or tool calls (e.g., GLM-4-AllTools)

#### What happens if the context window limit is exceeded?

- **Truncation of excess content**: If the total text volume exceeds the context window limit, the excess will be automatically discarded and cannot be processed.
- **Impact on conversation content**: You may not see the discarded parts, affecting the quality of the model's answers or the coherence of the context.

Check the model's context limitations or use the Tokenizer tool to estimate context length.

<Note>
Note: The above content only applies to GLM-4 series language models. For multimodal models, input content has strict length limitations. If exceeded, the system will prompt "prompt too long".
</Note>