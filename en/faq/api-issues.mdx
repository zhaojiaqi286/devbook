---
title: "API Issues"
description: "Common questions and solutions for Z.AI platform API calls"
---

# API Issues

This page summarizes common questions and solutions encountered by users when calling Z.AI large model open platform APIs.

## Basic Calls

### How to call our API?

You can refer to the interface documentation provided by our platform for calls.

**Call Steps:**
1. Register and log in to the platform
2. Obtain API key
3. View interface documentation
4. Write call code
5. Send request for testing

## Call Methods

### What's the difference between synchronous, asynchronous, and SSE call methods?

Synchronous, asynchronous, and SSE calls are three different API response methods.

**SSE Call (Recommended):**
- After the client initiates a request, it can obtain the content generated by the model in real-time streaming until inference ends
- Similar to the typewriter effect on the Zhipu Qingyan APP
- Suitable for scenarios with high requirements for first response and response duration, such as intelligent customer service and conversational chat that directly interact with users
- We recommend you use SSE calls for better user experience

**Synchronous Call:**
- The client initiates a request, and the model returns the complete generation result at once after completing inference
- Suitable for batch processing scenarios

**Asynchronous Call:**
- After the client initiates a request, users need to call asynchronous interface result queries to check model processing status and inference results
- If processing is complete, model generation results can be obtained through the result query interface
- Suitable for business scenarios that are not sensitive to response time, such as batch data processing and batch article generation

## Concurrency Limits

### What are the concurrency limits when calling models?

You can refer to rate limits to understand current concurrency and how to increase your concurrency.

**Methods to increase concurrency:**
- Complete real-name authentication
- Upgrade account level
- Contact customer service to apply for increases

## Parameter Settings

### How to set temperature and top_p parameters?

In large language models, temperature and top_p parameters are used to adjust the diversity and quality of generated text.

**temperature parameter:**
- Used to control the randomness of model output results
- Value range: [0.0,1.0]
- Higher values generate more random text
- Lower values generate more stable text

**top_p parameter:**
- Used to control the probability distribution of words or phrases in model output results
- Value range: [0.0,1.0]
- Higher values allow the model to choose from more words or phrases, increasing output randomness
- Lower values allow the model to choose from fewer words or phrases, increasing output stability

**Usage Recommendations:**
- To get more creative and diverse answers, set temperature to a higher value or top_p to a higher value
- To get more stable and deterministic answers, set temperature to a lower value or top_p to a lower value
- You can adjust temperature or top_p parameters according to actual application scenarios, but don't adjust both parameters simultaneously

## Function Calling

### How to use function calling capabilities?

You can refer to the function calling usage documentation to understand the calling logic.

### Does the tools list support passing multiple functions?

The tools list supports passing multiple functions, but only one can be hit per call.

### Can function calling, knowledge base retrieval, and web search all be added to the tools parameter?

Function calling, knowledge base retrieval, and web search are three mutually exclusive features. If used simultaneously, only one will take effect according to priority.

**Priority order:**
Function calling > Knowledge base retrieval > Web search

## Model Fine-tuning

### How to do model fine-tuning?

Currently, you can obtain GLM-4-Flash model fine-tuning permissions by submitting model fine-tuning interface documentation developer Pro version platform service benefits application. Fine-tuning capabilities for other models will be iteratively updated. After obtaining permissions, you can understand call details through the model fine-tuning interface documentation.

You can also purchase our cloud privatization services to get privatization deployment and model fine-tuning services. Please contact us anytime, and our consulting advisors will provide detailed introductions.

**Fine-tuning Process:**
1. Apply for fine-tuning permissions
2. Prepare training data
3. Submit fine-tuning tasks
4. Wait for training completion
5. Test fine-tuned model
6. Deploy and use